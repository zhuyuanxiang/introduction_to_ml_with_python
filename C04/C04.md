# Ch04 数据表示与特征工程

-   连续特征 ( Continuous Feature ) 。
-   分类特征 ( Categorical Feature ) ，也叫离散特征 ( Discrete Feature ) 。

特征工程：找到最佳的数据表示。

虽然神经网络已经可以自动完成特征的选择和变换，但是深入学习特征变换的原理，可以帮助理解神经网络对数据进行了哪种变化，从而加深对数据的理解，能够更加显式地操纵数据变换过程，提高数据变换的效率和加入更多的先验信息。

## 4.1 分类变量

### 4.1.1 One-Hot 编码 ( 虚拟变量 )

One-Hot 编码，也叫 N 取一编码 ( one-out-of-N encoding ) ，也叫虚拟变量 ( dummy variable ) 。

虚拟变量的思想：将一个分类变量替换为一个或者多个新的特征，新特征取值为0或者1。

### 4.1.2 数字可以编码分类变量

分类变量被编码为字符串，分类变量通常被编码为整数。

## 4.2 分箱、离散化、线性模型与树

特征分箱，也叫离散化，可以使线性模型在处理连续数据时的效果更好。

## 4.3 交互特征与多项式特征

添加原始数据的交互特征 ( Interaction Feature ) 和多项式特征 ( Polynomial Feature )
可以丰富特征表示。

为数据集和模型的所有组合寻找最佳变换是一门艺术。

-   对于复杂度较低的模型 ( 线性模型和朴素贝叶斯模型 ) ，数据变换提高模型的精确度；
-   对于复杂度较高的模型 ( 随机森林和梯度提升树 ) ，数据变换可能会降低模型的精确度，因为模型自己可以发现重要的交互项，人工的数据变换反而会影响数据的原有分布情况。
-   对于其他模型 ( SVM、最近邻、神经网络 ) ，数据变换有的时候也能够改善模型的性能，只是效果不如线性模型那么显著。

## 4.4 单变量非线性变换

基于树的模型只关注特征的顺序，但是线性模型和神经网络则依赖于每个特征的尺度和分布。

-   如果在特征和目标之间存在非线性关系，那么建模就变得困难，使用 log 或者  exp 函数就可以调节数据的相对比例，从而改进模型的学习效果，特别是对于回归问题帮助更大。
-   如果处理具有周期性模式的数据时，sin 和 cos 函数则非常有用。

大部分模型对于特征属于高斯分布时表现最好，在回归问题中还包括目标值属于高斯分布时表现最好

## 4.5 自动化特征选择

三种基本的策略：

-   单变量统计 ( Univariate Statistics ) ：计算每个特征和目标值之间的关系是否存在统计显著性，
然后选择置信度最高的特征。
    -   分类问题：叫做方差分析 ( Analysis of Variance, ANOVA ) ，只单独考虑每个特征。
    -   优点：计算速度快，不需要构建模型；完全独立于应用的模型
    -   删除噪声特征可以提高精确度，即使删除了部分原始特征，也不会影响模型的精确度。
    说明某些特征是有相关性的，删除部分特征后可以减少噪声的干扰。
-   基于模型的选择 ( Model-Based Selection ) ：使用一个监督机器学习模型来判断每个特征的重要性，
并且仅保留最重要的特征。
    -   特征选择模型不需要跟特征训练模型相同；
    -   特征选择模型需要提供特征的重要性度量，从而可以使用度量排序；
    -   特征选择模型同时考虑所有特征，可以获取交互项
    -   常用的特征选择模型：
        -   基于决策树的模型提供的 feature_importances_，可以直接编码每个特征的重要性
        -   线性模型系数的绝对值
-   迭代选择 ( Iterative Selection )
    -   两种迭代特征选择方法
        -   开始没有特征，逐渐增加特征，直到满足某个终止条件
        -   开始所有特征，逐渐删除特征，直到满足某个终止条件

特征选择总结：如果选择了正确的特征，线性模型的表现就能够与随机森林一样好。

## 4.6 利用专家知识 ( Expert Knowledge )

[自行车出租数据](https://www.citibikenyc.com/system-data)

通过对数据特征的不断优化，通过特征变换将专家的先验知识加入到数据中，可以显著提高模型的精确度。

通过对比不同的模型，还可以进一步了解模型处理数据时的分别。
