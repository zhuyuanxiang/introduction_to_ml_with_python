# Ch02 有监督学习

## 2.1 分类与回归

有监督学习：

-   分类 ( Classification ) ：目标是预测类别标签
    -   二分类 ( Binary Classification ) ：在两个类别之间进行区分
        -   正类 ( Positive Class )
        -   反类 ( Negative Class )
    -   多分类 ( Multiclass Classification ) ：在两个以上的类别之间进行区分
-   回归 ( Regression ) ：目标是预测一个连续值。
    -   编程术语叫做浮点数 ( Floatinig-point Number )
    -   数学术语叫做实数 ( Real Number )

## 2.2 泛化、过拟合与欠拟合

如果一个模型能够对没有见过的数据做出精确的预测，就说这个模型能够从训练集泛化 ( Generalize ) 到测试集。
因此，构建模型的目标是：泛化精度尽可能高。

过拟合 ( Overfitting ) ：模型复杂，得到一个在训练集上表现良好，在测试集上表现较差的模型。即模型的泛化性能较差，说明模型过拟合。

欠拟合 ( Underfitting ) ：模型简单，得到一个在训练集上表现较差的模型，说明模型欠拟合。

## 2.3 有监督学习算法

### 2.3.1 构建样本数据集

### 2.3.2 K 近邻算法

-   KNeighbors 分类器有 2 个重要参数
    -   邻居个数
    -   数据点之间距离的度量方法
-   优点：
    -   模型容易理解；
    -   不需要过多调节就可以获得好的性能
-   缺点：
    -   对于特征数目过多的数据集效果不好；
    -   对于特征取值多数为 0 的数据集 ( 稀疏数据集 ) 效果不好

### 2.3.3 线性模型：利用输入特征的线性函数进行预测

-   正则化：
    -   L1 正则化：给出更容易解释的模型，只选择了部分输入特征
    -   L2 正则化：
    -   Scikit-Learn 提供了 ElasticNet 类，结合了 Lasso 和 Ridge 的惩罚项
        -   优点：效果最好； ( 亲测了不觉得好在哪里 )
        -   缺点：需要调节两个参数。
-   线性分类模型：
    -   算法的区别：
        -   系数和截距的特定组合对训练数据拟合好坏的度量方法
        -   正则化方法
    -   算法的分类：
        -   Logistic 回归 ( Logistic Regression, LR )
        -   线性支持向量机 ( Linear Support Vector Machine, LSVM )
-   线性多分类模型：
    -   多分类算法：「一对其余」 ( One-vs-Rest ) 。
    每个类别都学习一个二分类模型，将这个类别与所有其他类别尽可能分开，这样就产生了与类别个数一样多的二分类模型，
    在测试点上运行所有的二分类模型进行预测，在对应类别上分数最高的分类器「胜出」作为预测结果。

### 2.3.4 朴素贝叶斯分类器

-   属于生成模型。训练速度更快，泛化能力稍差。
-   单独查看每个特征来学习参数，并且从每个特征中收集简单的类别统计数据。
-   Scikit-Learn 中实现了三种朴素 Bayes 分类器：
    -   GaussianNB：应用于任意的高维的连续数据；
        -   保存每个类别中每个特征的平均值和标准差
    -   BernoulliNB：假定输入数据为稀疏的二分类数据；
        -   计算每个类别中每个特征不为 0 的元素个数
        -   主要用于文本数据分类
    -   MultinomialNB：假定输入数据为计数数据 ( 即每个特征代表某个对象的整数计数，例如：一个单词在句子中出现的次数 )
        -   计算每个类别中每个特征的平均值
        -   主要用于文本数据分类

### 2.3.5 决策树

-   构造决策树
    -   使用有监督学习从数据中学习决策树
    -   对数据反复进行递归划分，直到划分后的每个区域 ( 决策树的每个叶结点 ) 只包含单一的目标值 ( 单一类别或者单一回归值 ) ，
    那么这个叶结点就很纯粹 ( Pure )
    -   决策树也可以应用于回归任务。
        -   基于每个结点的测试对树进行遍历，最终找到新数据点所属的叶结点。
        -   「新数据点」的输出为叶结点上所有训练点的平均目标值
-   控制决策树的复杂度
    -   过拟合的两种策略：
        -   预剪枝 ( Pre-pruning ) ：及早停止树的生长
            -   预剪枝的限制条件：
                -   限制树的最大深度
                -   限制叶结点的最大数目
                -   规定一个结点中数据的最小数目来防止继续划分
        -   后剪枝 ( Post-pruning ) ：也叫剪枝。先构造树，随后删除或者折叠信息量较少的「结点」
-   分析决策树：
    -   在 sklearn.tree 模块中 export_graphviz ( ) 函数提供了树的可视化方法。
    -   生成的 PDF 文件中方框内的含义：
        -   第一行是数据判别的条件
        -   第二行：samples 表示样本个数
        -   第三行：value=[25,50] 表示正负样本的个数分别是多少
-   树的特征重要性：
    -   特征重要性求和为 1。
    -   每个特征的重要性数值在 0 与 1 之间。
-   优点
    -   得到的模型容易可视化
    -   算法不受数据缩放的影响
-   缺点
    -   即使做了预剪枝，仍然容易过拟合，泛化能力较差。
    -   使用决策树集成方法来代替单棵决策树。

### 2.3.6 决策树集成

-   集成 ( Ensemble ) 是合并多个机器学习模型来构建更加强大模型的方法。
-   集成模型的分类：
    -   随机森林 ( Random Forest, RF )
        -   本质上是许多棵决策树的集合，其中每棵树都和其他树略有不同
        -   思想是每棵树的预测可能都相对比较好，但是都可能对部分数据过拟合。
        如果构造许多棵树，对这些树的结果取平均值来降低过拟合。
        -   数学上严格证明了：随机森林方法既能减少过拟合，还能保持树的预测能力。
        -   树的随机化方法：
            -   通过选择用于构造树的数据点
            -   通过选择每次划分测试的特征
        -   构造随机森林模型
            -   首先，确定用于构造的树的个数
            -   其次，对数据进行自助采样，使得每棵树的数据集合尽可能不同
            -   然后，基于采样的数据分别构造决策树
        -   优点： 可以并行计算，效果比较好，不需要反复调节参数，也不需要对数据进行缩放。
        -   缺点： 计算对于时间和空间的需要较大。

    -   梯度提升决策树 ( Gradient Boosted Decision Tree, GBDT ) ，也叫梯度提升机。

### 2.3.7. 核支持向量机 ( Kernelized Support Vector Machine )

-   SVC 中实现分类问题。
-   SVR 中实现回归问题。

1.  线性模型与非线性特征
2.  核技巧 ( Kernel Trick )
    -   核技巧：直接计算扩展特征表示中数据点之间的距离 ( 即内积 ) 。
    -   常用核：多项式核；高斯核 ( 径向基函数核 )
3.  理解 SVM
    -   在训练过程中，SVM 学习每个训练数据点对于表示两个类别之间的决策边界的重要性。
    -   只有一部分训练数据点对于定义决策边界很重要：位于类别之间边界上的点叫做支持向量。
    -   分类决策面基于边界与支持向量之间的距离以及在训练过程中学到的支持向量的重要性来做出的。
4.  SVM 调参
5.  优点、缺点和参数
    -   优点：核支持向量机可以在各种数据集上都表现良好。允许决策边界任意复杂。
    -   缺点：对于数据集中尺度相关较大的特征数据表现不好；需要归一化数据；需要人工调整参数
    -   重要参数：
        -   正则化参数 C；
        -   核的选择；
        -   RBF 核参数 gamma，高斯核宽度的倒数

### 2.3.8. 神经网络 ( 深度学习 )

scikit-learn 不是比较好的神经网络工具。
建议了解 keras、lasagna、Tensorflow。因为它们可以利用 GPU 进行加速计算，而 scikit-learn 不能使用 GPU 加速计算。
    -   lasagna 是基于 theano 库构建的；
    -   keras 既可以用于 Tensorflow，也可以用于 theano。

1.  优点、缺点和参数
    -   优点：获取大量数据中包含的信息，构建复杂的模型
    -   缺点：需要很长的训练时间，需要预处理数据，需要仔细地调整参数
    -   参数：创建大到过拟合的网络，缩小网络或者增大正则化参数来提高泛化性能。
        -   主要关注的参数：网络层数、每层的结点个数、正则化和激活函数的非线性化。

## 2.4. 分类器的不确定度估计 ( 预测的置信程度 )

对于二分类问题

-   「反」类是 classes_属性的第一个元素
-   「正」类是 classes_属性的第二个元素

### 2.4.1 决策函数

小于阈值的代表反类，大于阈值的代表正类。

### 2.4.2 预测概率

概率大的类别是预测结果。

### 2.4.3 多分类问题的不确定度估计

1.  决策函数：决策函数输出值大的类别代表预测结果
2.  预测概率：预测概率输出值大的类别代表预测结果

## 2.5 小结与展望

重点概念：

-   模型复杂度
-   泛化
    -   欠拟合：模型没有获取训练数据中的所有变化
    -   过拟合：模型对训练数据的所有变化拟合得过好，导致将噪声的变化也纳入模型，
    从而无法有效推广到新数据中。

常用模型总结：先从简单模型开始，了解数据后，再构建复杂模型。

-   简单模型
    -   最近邻：适用于小型数据集，是很好的基准模型，容易解释
    -   线性模型：可靠的首先算法，适用于非常大的数据集，也适用于高维数据
    -   朴素贝叶斯：只适用于分类问题，比线性模型速度快，适用于非常大的数据集，
    也适用于高维数据，精度通常要低于线性模型。
    -   决策树：速度快，不需要数据缩放，可以可视化，容易解释
-   复杂模型
    -   随机森林：比单棵决策树表现要好，鲁棒性好，不需要数据缩放，不适用于高维稀疏数据
    -   梯度提升决策树：与随机森林相比，精度高，训练速度慢，预测速度快，需要的内存少，调节的参数多。
    -   支持向量机：对于特征含义相似的中等大小的数据集效果最好，需要数据预处理 ( 缩放 ) ，对参数敏感
    -   神经网络：可以构建复杂的模型，适用于大型数据集，需要数据预处理 ( 缩放 ) ，对参数敏感，训练时间长
