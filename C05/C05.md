# Ch05 模型评估与改进

模型评估：
- 交叉验证：可靠的评估泛化性能的方法

模型改进：
- 网格搜索是一种调节监督模型参数以获得最佳泛化性能的有效方法。

## 5.1 交叉验证

交叉验证（Cross-Validation）：是一种评估泛化性能的统计学方法，比单次划分训练集和测试集的方法更加稳定和全面。

最常用的交叉验证方法：K折交叉验证（K-fold cross validation)

### 5.1.1 Scikit-Learn中的交叉验证

from sklearn.model_selection import cross_val_score

### 5.1.2 交叉验证的优点

优点：
- 相比于单次划分，数据分布更加随机
- 相比于单次划分，数据利用更加充分

缺点：增加了计算成本

### 5.1.3 Scikit-Learn中的交叉验证的各种策略

- K折交叉验证（KFold）｛最常用！｝
    - 分层K折交叉验证（StratifiedKFold）
    - 重复分层K折交叉验证（RepeatedStratifiedKFold）
    - 分组K折交叉验证（GroupKFold）
- 留P法（LeavePOut)
    - 留P组数据（LeavePGroupsOut）
    - 留一法（LeaveOneOut）
    - 留一组数据（LeaveOneGroupOut）
- 打乱划分交叉验证（ShuffleSplit）
    - 分层打乱划分交叉验证（StratifiedShuffleSplit）
    - 分组打乱划分交叉验证（GroupShuffleSplit）

## 5.2 网格搜索

网格搜索（grid search）：从参数的所有可能组合中找出最佳的参数设置。

### 5.2.1 简单的网格搜索

使用循环对所有参数组合分别训练并且评估一个分类器。

### 5.2.2 参数过拟合和验证集校正

如果通过测试集的评估来修正模型训练使用的参数，
那么最终评估模型泛化能力时会因为评估过程的反复修正而导致泛化能力被高估。
如果增加验证集来修正模型训练使用的参数，
那么最终评估模型泛化能力时，因为使用的是未知数据，所以得到的泛化性能评估更加可信。
利用测试集修正模型参数导致的误差称作将测试集的信息“泄漏”到模型中。

### 5.2.3 带交叉验证的网格搜索

小数据集带来的过拟合问题依然没有很好解决，因此使用交叉验证来评估每种参数组合的性能，
可以得到对泛化性能的更好的估计。

1. 从相对比较稀疏并且比较小的网格开始搜索，通过分析交叉验证的结果，找出合适的参数范围
- 将交叉验证的结果可视化有助于理解模型泛化能力对所搜索参数的依赖关系
2. 在非网格的空间中搜索
3. 使用不同的交叉验证策略进行网格搜索
    - （1）嵌套交叉验证：使用交叉验证对原始数据进行多次划分。
        - 外层循环，将原始数据划分为训练集和测试集多种划分
        - 内层循环，对于每种外层划分，利用最佳参数设置计算得到测试集分数。
    - （2）交叉验证与网格搜索并行
        - n_jobs参数可以设置使用多个CPU内核
        - Scikit-Learn不允许并行操作的嵌套。
        例如：随机森林中使用了n_jobs参数，就不能在GridSearchCV中使用n_jobs参数。
        - IPython并行框架可以进行并行网格搜索
        - spark-sklearn允许在建立好的Spark集群上运行网格搜索

## 5.3 评估指标与评分

- 分类性能评估：精度（正确分类的样本所占的比例）
- 回归性能评估：R^2

### 5.3.1 评估的最终目标

- 技术服务于实践。实践中的指标才是关键。

### 5.3.2 二分类问题的评估指标

1. 错误类型：
    - 正类（Positive Class）
        - 真正例（True Positive）：正确的阳性预测。
        - 假正例（False Positive）：错误的阳性预测，第一类错误。
    - 反类（Negative Class）
        - 真反例（True Negative）：正确的阴性预测。
        - 假反例（False Negative）：错误的阴性预测，第二类错误。
2. 不平衡的数据集（Imbalanced Dataset），
也叫具有不平衡类别的数据集（Dataset with Imbalanced Classes），
数据集中一个类别比另一个类别出现的次数多得多。
3. 混淆矩阵（Confusion Matrix）
- 混淆矩阵与精度：Accuracy=（TP+TN）/（TP+TN+FP+FN）；所有样本中正确预测样本的比例。
- 混淆矩阵与准确率：Precision=TP/（TP+FP）；被预测为正例的样本中正确正例的比例，
也叫阳性预测值（Positive Predictive Value，PPV）。
- 混淆矩阵与召回率：Recall=TP/（TP+FN）；所有正类样本中正确正例的比例，
也叫灵敏度（Sensitivity）、命中率（Hit Rate）和真正例率（True Positive Rate，TPR）。
- 混淆矩阵与F-度量：F=2（Precision-Recall）/（Precision+Recall）；准确率与召回率的调和平均，
也叫F-分数、F_1-分数。
    - F-度量比精度更加符合我们对好模型的评价，但是相对于精度也更加难以解释。
4. 考虑不确定性
- “宏”（Macro）平均：计算未加权的按类别f-分数。对所有类别给出相同的权重，无论类别中的样本量大小。
- “加权”（Weighted）平均：以每个类别的支持作为权重来计算按类别f-分数的平均值。分类报告中给出的就是这个值。
- “微”（Micro）平均：计算所有类别中假正例、假反例和真正例的总数，然后利用这些计数来计算准确率、召回率和f-分数。
5. 准确率-召回率曲线（Precisioin-Recall Curve）：帮助寻找合适的工作点（Operating Point）。
6. 受试者工作特征（Receiver Operating Characteristics Curve，ROC）
与 曲线下的面积（Area Under the Curve，AUC）
- ROC：显示的是假正例率和真正例率

### 5.3.3 多分类问题的评估指标

- 混淆矩阵
- 分类报告
- f-分数

### 5.3.4 回归问题的评估指标

R^2是评估回归模型的标准指标。

### 5.3.5 模型选择问题的评估指标

使用GridSearchCV或者cross_val_score进行模型选择时能够使用AUC等指标。

## 5.4 小结与展望

- 交叉验证或者使用测试集可以评估一个机器学习模型的泛化能力
- 模型选择与模型评估的评估指标或者评分函数可以评估商业决策