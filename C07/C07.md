# Ch07 处理文本数据

## 7.1 用字符串表示的数据类型

四种类型的字符串数据：
- 分类数据（Categorical Data）是来自固定列表的数据
- 可以在语义上映射为类别的自由字符串：利用最常见的条目来选择类别，可以自定义类别，
使用户回答对应用有意义。
- 结构化字符串数据：数据存在内存的结构，这种类型的字符串难以解析，其处理方法依赖于上下文和具体领域。
- 文本数据：是自由格式，由短语和句子组成。

在文本分析的语境中：
- 数据集被称为语料库（Corpus）
- 每个由单个文本表示的数据点称为文档（Document）
- 文本分析属于信息检索（Information Retrieval，IR）和自然语言处理（Natural Language Processing，NLP）

## 7.2 示例应用：电影评论的情感分析

- 使用由Stanford的Andrew Maas收集的
互联网电影资料库（Internet Movie Database，IMDb）网站的电影评论数据集。

## 7.3 将文本数据表示为词袋

词袋（bag-of-words）表示是用于机器学习的文本表示中最简单、最有效、最常用的方法。
用这种表示方式时，舍弃了输入文本中的大部分结构，只计算语料库中每个单词每个文本中的出现频次。

计算词袋的三个步骤：
1. 分词（tokenization）：将每个文档划分为出现在其中的单词｛称为词例（token）｝，比如：按照空格和标点划分。
2. 构建词表（vocabulary building）：收集一个词表，里面包含出现在输入文本中的所有词，并对它们进行编号，比如：按照字母顺序排序
3. 编码（encoding）：对于每个文档，计算词表中每个单词在该文档中的出现频次

### 7.3.1 将词袋应用于简单的数据集

词袋表示使用CountVectorizer类实现，这个类是一个变换器（transformer）。

### 7.3.2 将词袋应用于电影评论

为了减少特征，可以删除信息量比较小的单词。
一种方法是设置min_df(5)表示至少出现在5个文档中的单词才被统计，
可以有效地减少数字、生僻单词和拼写错误的单词。也可以加速模型的处理速度，提高模型的可解释性。
但是对于提高模型的精度没有帮助。

## 7.4 停用词（stopword）

另一种删除信息量比较小的单词的方法是：删除停用词。
- 特定停用词列表。

还可以设置max_df(100)表示至多出现在100个文档中的单词才被统计。可以加速模型的处理速度，但是会降低模型的精度。

## 7.5 用TF-IDF缩放数据

不再删除那些被认作不重要的单词，而是按照预计的特征信息量大小来缩放特征。
- 词频--逆向文档频率（Term Frequency-Inverse Document Frequency，TF-IDF）方法。
对在某个文档中经常出现的术语给予高权重，对于在语料库的许多文档中经常出现的术语给予低权重。
因为在某个文档中经常出现，而在许多文档中不经常出现的术语，可能对文档的内容有很好的描述。
$tfidf(w,d)=tf log(\frac{N+1}{N_w+1)) +1$
- N是训练集中的文档数量
- N_w是训练集中出现单词w的文档数量
- tf（词频）是单词w在查询文档d中出现的次数

## 7.6 研究模型的系数

由于特征数量（单词数量）非常多，不能同时查看所有系数，但是可以查看最大系数，并且查看这些系数对应的单词。

## 7.7 多个单词的词袋（N元分词）

使用词袋表示导致放弃了单词顺序（即上下文）。
-   一元分词（Unigram）
-   二元分词（Bigram）
-   三元分词（Trigram）：最常用。
-   N元分词（N-gram）

## 7.8 词干提取和词形还原

单词的标准化：提取一个单词的某种标准形式。
-   词干提取：基于规则的启发式方法实现
-   词形还原：基于已知单词形式的词典来实现。
    -   词形还原对精度的提高比较有限，在特定任务中先使用最简单的模型，然后再使用更多的技术来提升模型精度。
    
## 7.9 主题建模和文档聚类

主题建模（Topic Modeling）：将每个文档分配给一个或者多个主题，通常采用无监督技术。
例如：新闻数据，可以被分为“政治”、“体育”、“金融”等主题。
- 如果每个文档分配一个主题，那么可以定义为文档聚类任务。类似：K-Means
- 如果每个文档分配多个主题，那么可以定义为成分分解任务。类似：PCA

LDA（Latent Dirichlet Allocation，隐含的锹利克雷分配模型）：
试图找出频繁的共同出现的单词群组（即主题）。
每个文档可以被理解为多个主题的“混合”，主题类似于成分，可能具有语义，也可能没有语义。

LDA对于理解大型文本语料库是有帮助的。
但是LDA算法是随机的，改变random_state参数可能会得到完全不同的结果。
虽然找到的主题可能有用，但是对于从无监督模型中得到的结论都应该持保留态度。
可以通过查看特定主题中的文档来验证自己对文档的分析。
LDA.transform()函数生成的主题有时也可以用于监督学习的紧凑表示，特别是在训练样例很少时，这个方法会有帮助。

## 7.10 小结与展望

- NLTK: "Natural Language Processing with Python"
- IR: "Introduction to Information Retrieval"
- 使用连续向量表示，也叫做词向量（Word Vector）或者分布式词表示（Distributed Word Representation），
在Word2Vec库中实现。
" Distributed Representations of Words and Phrases and Their Compo sitionality"
- 递归神经网络（Recurrent Neural Network，RNN）进行文本处理，可以生成同样是文本的输出，适合自动翻译和摘要。
"Sequence to Sequence Learning with Neural Networks"
